{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI API key here and hit enter:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf\n",
    "!pip install tiktoken"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyPdfloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"cucm_b_troubleshooting-guide-1251-extracted (1).pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Davinvi v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "# create a GPT-4 encoder instance\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "total_word_count = sum(len(doc.page_content.split()) for doc in pages)\n",
    "total_token_count = sum(len(enc.encode(doc.page_content)) for doc in pages)\n",
    "\n",
    "print(f\"\\nTotal word count: {total_word_count}\")\n",
    "print(f\"\\nEstimated tokens: {total_token_count}\")\n",
    "print(f\"\\nEstimated cost of embedding: ${total_token_count * 0.2 / 1000}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ada v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word_count = sum(len(doc.page_content.split()) for doc in pages)\n",
    "total_token_count = sum(len(enc.encode(doc.page_content)) for doc in pages)\n",
    "\n",
    "print(f\"\\nTotal word count: {total_word_count}\")\n",
    "print(f\"\\nEstimated tokens: {total_token_count}\")\n",
    "print(f\"\\nEstimated cost of embedding: ${total_token_count * 0.0004  / 1000}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ada v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word_count = sum(len(doc.page_content.split()) for doc in pages)\n",
    "total_token_count = sum(len(enc.encode(doc.page_content)) for doc in pages)\n",
    "\n",
    "print(f\"\\nTotal word count: {total_word_count}\")\n",
    "print(f\"\\nEstimated tokens: {total_token_count}\")\n",
    "print(f\"\\nEstimated cost of embedding: ${total_token_count * 0.0040  / 1000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Babbage v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word_count = sum(len(doc.page_content.split()) for doc in pages)\n",
    "total_token_count = sum(len(enc.encode(doc.page_content)) for doc in pages)\n",
    "\n",
    "print(f\"\\nTotal word count: {total_word_count}\")\n",
    "print(f\"\\nEstimated tokens: {total_token_count}\")\n",
    "print(f\"\\nEstimated cost of embedding: ${total_token_count * 0.0050  / 1000}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curie v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word_count = sum(len(doc.page_content.split()) for doc in pages)\n",
    "total_token_count = sum(len(enc.encode(doc.page_content)) for doc in pages)\n",
    "\n",
    "print(f\"\\nTotal word count: {total_word_count}\")\n",
    "print(f\"\\nEstimated tokens: {total_token_count}\")\n",
    "print(f\"\\nEstimated cost of embedding: ${total_token_count * 0.020  / 1000}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"cucm_b_troubleshooting-guide-1251-extracted (1).pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word_count = sum(len(doc.page_content.split()) for doc in pages)\n",
    "total_token_count = sum(len(enc.encode(doc.page_content)) for doc in pages)\n",
    "\n",
    "print(f\"\\nTotal word count: {total_word_count}\")\n",
    "print(f\"\\nEstimated tokens: {total_token_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = './db/'\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "vectordb = Chroma.from_documents(documents=pages, embedding= embeddings,persist_directory=persist_directory)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = './db/'\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
    "Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "----------------\n",
    "{summaries}\"\"\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "chain_type_kwargs = {\"prompt\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain import OpenAI\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(OpenAI(model_name=\"text-embedding-ada-002\",temperature=0), chain_type=\"stuff\", retriever=vectordb.as_retriever(),  return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "def print_result(result):\n",
    "  output_text = f\"\"\"### Question: \n",
    "  {query}\n",
    "  ### Answer: \n",
    "  {result['answer']}\n",
    "  ### Sources: \n",
    "  {result['sources']}\n",
    "  ### All relevant sources:\n",
    "  {' '.join(list(set([doc.metadata['source'] for doc in result['source_documents']])))}\n",
    "  \"\"\"\n",
    "  display(Markdown(output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Enable Email Alert for Core Dump', 'answer': ' 1.1\\n 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n 1, 1, 1, 1,\\n 1, 1, 1, 1,\\n\\n 1, 1, 1, 1, 1,\\n 1, 1,', 'sources': '', 'source_documents': [Document(page_content='Refer to the Troubleshooting Guide for Cisco Unified Communications Manager for more information on the\\nkerneldump utility and troubleshooting.\\nEnable Email Alert for Core Dump\\nUse this procedure to configure the Real-Time Monitoring Tool to email the administrator whenever a core\\ndump occurs.\\nProcedure\\nStep 1\\nSelect System > Tools > Alert > Alert Central.\\nStep 2\\nRight-click CoreDumpFileFound alert and select Set Alert Properties.\\nStep 3\\na)Follow the wizard prompts to set your preferred criteria:\\nIn the Alert Properties: Email Notification popup, make sure that Enable Email is checked and click\\nConfigure to set the default alert action, which will be to email an administrator.\\nb) Follow the prompts and Add a Receipient email address. When this alert is triggered, the default action\\nwill be to email this address.\\nc)\\nClick Save.\\nStep 4\\na)Set the default Email server:\\nSelect System > Tools > Alert > Config Email Server.\\nc)b) Enter the e-mail server settings.\\nClick OK.\\nNetwork Management\\nUse the network management tools for Unified Communications Manager remote serviceability.\\n• System Log Management\\n• Cisco Discovery Protocol Support\\n• Simple Network Management Protocol support\\nRefer to the documentation at the URLs provided in the sections for these network management tools for\\nmore information.\\nSystem Log Management\\nAlthough it can be adapted to other network management systems, Cisco Syslog Analysis, which is packaged\\nwith Resource Manager Essentials (RME), provides the best method to manage Syslog messages from Cisco\\ndevices.\\nCisco Syslog Analyzer serves as the component of Cisco Syslog Analysis that provides common storage and\\nanalysis of the system log for multiple applications. The other major component, Syslog Analyzer Collector,\\ngathers log messages from Unified Communications Manager servers.\\nTroubleshooting Guide for Cisco Unified Communications Manager, Release 12.5(1)\\n9\\nTroubleshooting Tools\\nEnable Email Alert for Core Dump', metadata={'source': 'cucm_b_troubleshooting-guide-1251-extracted (1).pdf', 'file_path': 'cucm_b_troubleshooting-guide-1251-extracted (1).pdf', 'page': 8, 'total_pages': 29, 'format': 'PDF 1.5', 'title': 'Troubleshooting Guide for Cisco Unified Communications Manager, Release 12.5(1)', 'author': 'CTG', 'subject': '', 'keywords': '', 'creator': 'DITA Open Toolkit', 'producer': 'GPL Ghostscript 9.52', 'creationDate': \"D:20230530084658+02'00'\", 'modDate': \"D:20230530084658+02'00'\", 'trapped': ''})]}\n"
     ]
    }
   ],
   "source": [
    "query = \"Enable Email Alert for Core Dump\"\n",
    "result = chain(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the pdf and storing the data in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def read_pdf(pdf_file):\n",
    "  \"\"\"Reads a PDF file and returns the text content.\n",
    "\n",
    "  Args:\n",
    "    pdf_file: The path to the PDF file to read.\n",
    "\n",
    "  Returns:\n",
    "    The text content of the PDF file.\n",
    "  \"\"\"\n",
    "\n",
    "  with open(pdf_file, 'rb') as f:\n",
    "    pdf_reader = PyPDF2.PdfReader(f)\n",
    "    text = ''\n",
    "    for page in pdf_reader.pages:\n",
    "      text += page.extract_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  pdf_file = 'cucm_b_troubleshooting-guide-1251-extracted (1).pdf'\n",
    "  text = read_pdf(pdf_file)\n",
    "\n",
    "  print(text)\n",
    "\n",
    "  file1=open(r\"./cucm_b_troubleshooting-guide-1251-extracted.txt\",\"a\")\n",
    "file1.writelines(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "doc = fitz.open('cucm_b_troubleshooting-guide-1251-extracted (1).pdf')\n",
    "text = \"\"\n",
    "for page in doc:\n",
    "   text+=page.get_text()\n",
    "file1=open(r\"./cucm_b_troubleshooting-guide-1251-extracted.txt\",\"a\")\n",
    "file1.writelines(text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CharacterTextSplitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main model using chatgpt 3.5 turbo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "doc = textract.process(\".\\cucm_b_troubleshooting-guide-1251-extracted (1).pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert PDF to text\n",
    "import textract\n",
    "doc = textract.process(\"./cucm_b_troubleshooting-guide-1251-extracted (1).pdf\")\n",
    "\n",
    "# Step 2: Save to .txt and reopen (helps prevent issues)\n",
    "with open('./cucm_b_troubleshooting-guide-1251-extracted.txt', 'w') as f:\n",
    "    f.write(doc.decode('utf-8'))\n",
    "\n",
    "with open('./cucm_b_troubleshooting-guide-1251-extracted.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 24,\n",
    "    length_function = len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_texts(chunks, embeddings, metadatas=[{\"source\": f\"{i}-pl\"} for i in range(len(chunks))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER\\n\\n1\\n\\nTroubleshooting Overview\\n\\nThis section provides the necessary background information and available resources to troubleshoot the Cisco\\n\\nUnified Communications Manager.\\n\\n• Cisco Unified Serviceability, on page 1\\n\\n• Cisco Unified Communications Operating System Administration, on page 2\\n\\n• General Model of Problem Solving, on page 2\\n\\n• Network Failure Preparation, on page 3\\n\\n• Where to Find More Information, on page 3\\n\\nCisco Unified Serviceability\\n\\nCisco Unified Serviceability, a web-based troubleshooting tool for Unified Communications Manager, provides\\n\\nthe following functionality to assist administrators troubleshoot system problems:\\n\\n• Saves Unified Communications Manager services alarms and events for troubleshooting and provides\\n\\nalarm message definitions.\\n\\n• Saves Unified Communications Manager services trace information to various log files for troubleshooting.\\n\\nAdministrators can configure, collect, and view trace information.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = './db/'\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(OpenAI(temperature=0), chain_type=\"stuff\", retriever=vectordb.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
    "Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "----------------\n",
    "{summaries}\"\"\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': ' To verify if the service is currently running, go to Unified Communications Manager Administration, choose Navigation > Cisco Unified Serviceability, choose Tools > Control Center – Feature Services, and from the Servers column, choose the server. The Status column will display which services are running for the chosen server.\\n',\n",
       " 'sources': 'cucm_b_troubleshooting-guide-1251-extracted (1).pdf'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"question\": \"how to t to verify if the service is currently running.\"}, return_only_outputs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# metadatas = [ {\"source\" : \"Troubleshooting Guide for Cisco Unified Communications Manager, Release 12.5(1)\"}]\n",
    "texts = text_splitter.create_documents([cucm_b_troubleshooting_guide])\n",
    "print(texts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = text_splitter.split_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "\n",
    "total_word_count = sum(len(doc.page_content.split()) for doc in split_docs)\n",
    "total_token_count = sum(len(enc.encode(doc.page_content)) for doc in split_docs)\n",
    "\n",
    "print(f\"\\nTotal word count: {total_word_count}\")\n",
    "print(f\"\\nEstimated tokens: {total_token_count}\")\n",
    "print(f\"\\nEstimated cost of embedding: ${total_token_count * 0.0004 / 1000}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vector Store using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = './db/'\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "vectordb = Chroma.from_documents(documents=split_docs, embedding= embeddings,persist_directory=persist_directory)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Audit Logging\"\n",
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
    "Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "----------------\n",
    "{summaries}\"\"\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain import OpenAI\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(OpenAI(model_name=\"text-embedding-ada-002\",temperature=0), chain_type=\"stuff\", retriever=vectordb.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Question: \n",
       "  verify which Cisco CallManager services are active\n",
       "  ### Answer: \n",
       "   To enable an email alert for a core dump, select System > Tools > Alert > Alert Central, right-click CoreDumpFileFound alert and select Set Alert Properties, follow the wizard prompts to set your preferred criteria, and set the default Email server.\n",
       "Source: 17-pl, 16-pl, 14-pl, 15-pl.\n",
       "\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# chain({\"question\": \"Enable Email Alert for Core Dump\"}, return_only_outputs=True)\n",
    "result = chain({\"question\": \"Enable Email Alert for Core Dump\"}, return_only_outputs=True)\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "def print_result(result):\n",
    "  output_text = f\"\"\"### Question: \n",
    "  {query}\n",
    "  ### Answer: \n",
    "  {result['answer']}\n",
    "\n",
    "  \"\"\"\n",
    "  display(Markdown(output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.create_documents([text])\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text = text_splitter.split_text(text)\n",
    "split_text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "total_word_count = sum(len(doc.page_content.split()) for doc in split_docs)\n",
    "total_token_count = sum(len(enc.encode(doc.page_content)) for doc in split_docs)\n",
    "\n",
    "print(f\"\\nTotal word count: {total_word_count}\")\n",
    "print(f\"\\nEstimated tokens: {total_token_count}\")\n",
    "print(f\"\\nEstimated cost of embedding: ${total_token_count * 0.0004 / 1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
