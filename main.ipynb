{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI API key here and hit enter:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert PDF to text\n",
    "import textract\n",
    "doc = textract.process(\"./cucm_b_troubleshooting-guide-1251-extracted (1).pdf\")\n",
    "\n",
    "# Step 2: Save to .txt and reopen (helps prevent issues)\n",
    "with open('./cucm_b_troubleshooting-guide-1251-extracted.txt', 'w') as f:\n",
    "    f.write(doc.decode('utf-8'))\n",
    "\n",
    "with open('./cucm_b_troubleshooting-guide-1251-extracted.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 24,\n",
    "    length_function = len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "embeddings = OpenAIEmbeddings()\n",
    "persist_directory = './db/'\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "# documents = []\n",
    "# for chunk in chunks:\n",
    "# #  document = Document(page_content=chunk, metadata={\"source\": f\"{i}-pl\"} for i in range(len(chunks))])\n",
    "#  documents.append(document)\n",
    "\n",
    "# vectordb = Chroma.from_documents(documents=documents, embedding= embeddings,persist_directory=persist_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectordb = Chroma.from_texts(chunks, embeddings, metadatas=[{\"source\": f\"{i}-pl\"} for i in range(len(chunks))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
    "Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "----------------\n",
    "{summaries}\"\"\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_type_kwargs = {\"prompt\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain import OpenAI\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(OpenAI(model_name=\"gpt-3.5-turbo\",temperature=0), chain_type=\"stuff\", retriever=vectordb.as_retriever(),  return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"give the complete procedure  for Extracting and analyzing pertinent data includes performing the following tasks?\"\n",
    "result = chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give the complete procedure  for Extracting and analyzing pertinent data includes performing the following tasks?:\n",
      "To extract and analyze pertinent data, the following tasks should be performed:\n",
      "1. Add end users to the Standard Packet Sniffer Users group.\n",
      "2. Configure packet capturing service parameters in the Service Parameter Configuration window in Unified Communications Manager Administration; for example, configure the Packet Capture Enable service parameter.\n",
      "3. Configure packet capturing settings on a per-device basis in the Phone or Gateway or Trunk Configuration window.\n",
      "4. Capture SRTP packets by using a sniffer trace between the affected devices.\n",
      "5. Contact Cisco Technical Assistance Center (TAC) directly after gathering the Packet Capture File, Key for the file, and user name and password of end user that belongs to the Standard Packet Sniffer Users group.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result['question']+\":\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
